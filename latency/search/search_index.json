{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":true,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"How to measure Latency? Heinrich Hartmann - P99 CONF - 2021-10-06 Introduction Inspiration for this talk comes from Gil Tene - How (NOT) to Measure Latency? Slides (London 2013) / Video (StrangeLoop 2015) / Blog - HighScalability 2015 I have been talking about Statistics and Lantency for the last years: State of the Histogram (SLOConf 2021) / Circllhist (paper) / Latency SLOs Done Right (FOSDEM 2019) / Statistics for Enginees (2014..2019) Plan for Today How to measure Latency? Where to measure Latency? How to store Latency Data? How to analyze Latency Data? Applications Montioring Benchmarking How to Measure Latency? Measuring Latency in code is relatively straight forward: t_start = time . now () # # code section you want to measure # latency = time . now () - t_start This is really all there is to it. There are a few things to watch out: Capture early returns / exceptions. Use: defer , try/catch/finally . Which clock is used? What Resolution? sec / ms / usec / ns Monotonic or affected by NTP resets? What is measured? System Time, Process Time Want: high resolution, monotonic, system time: Java System.nanoTime() C clock_gettime(CLOCK_MONOTONIC,...) / gethrtime Python time.monotonic() Measurement Overhead. Checking the time costs time. Depending on implementation this can be 30ns - 300ns for the C-level call 1 , plus function call overhead of the language runtime. Rule of thumb: OK to measure latencies of 0.1ms or larger. If you are taking more than three time measurements, it's likely a good idea to abstract the time measurement into an object or a decorator, that can be used as follows: @timed def do_some_things (): ... Some tracing libraries (e.g. OpenTelemetry ) provide such decorators out-of-the box. Measuring Latency over Time When measuring latency over time, e.g. in a monitoring system, there are trickly interactions of the collection window with the request duration that we are capturing: Do we count processes that overlap the collection windows? IMG: Latencies over Time Best Practices: Count requests to the collection window when they were completed When shutting down the benchmark, wait for all requests to complete! Keep an eye on the active requests (metric) Capture max latency, if this is larger than collection window, we make serious mistakes. Option: count active requests with the elapsed duration at end-of-window. Where to Measure Latency? This is the much trickier part! We are most interested in measuring timings of request-reply interactions in potentially distributed system (HTTP/gRPC API calls): +--------+ +-----------+ +-----------+ +--------+ | Client | -----> | Intermedy | > | Intermedy | ------> | Server | +--------+ +-----------+ +-----------+ +--------+ The latency that the client experiences is influenced by the whole chain of interactions. Tradeoff: Measuring close to client is more meaningful but hard Measuring at server is easy but can be extermely misleading Best: Measure Both -> Tracing. IMG: Server Span / Client Span Hidden Queues The computer systems we are working with have a lot of hidden queues in the HTTP libraries, TCP/IP stack, NIC Cards, Network Gear, etc. this means that the latency that the client experiences may be much worse, than what can be measured on the server side. It's very hard to observe those queues directly, we have to measure at the client to make sure we capture the impacts of all of them. IMG: Hidden Queues We can model these systems as queuing systms with \"hidden\" queues. Request Time vs. Service Time It's very easy to confuse request time with service time. If you are measing latency at the server, you are measuring service time. Serice Time looks like Request Time when there is little traffic Service Time can look completely FINE even if your clients have TERRIBLE experience. In Queuing Systems, this can have drastic consequences. IMG: Server with Queue Let's do some analysis... Coordinated Omission in Benchmarking In benchmarking applications there are are a surprising number of cases, where the load generator coordinates with the system under test, and thereby skews latency measurements. Examples: Load is generated in loop like so: while True : start = time . now () request . get ( \"some.api/endpoint\" ) record ( time . now () - start ) This logic backs-up as soon as the requests take longer. You are effectively measuring service-time of a single worker. When latency is dominated by GC activity (e.g. Java) and the load generator runs in the in the same process, it will be affected by GC pauses as well, and hence miss requests when the system is stalled. How to Store Latency Data? Requirements: Can compute percentiles Can merge latency data over longer time periods Options: Raw Data (logs, events, traces) Histograms (i.e. Mergable Summaries, Quantile Digest, etc.) How to Analyze Latency Data Look at the max latency. Stalls of the system, often only affect a low number of requests. They still cause queuing in the upstream systems. Hence, when measuring service-time (which is often the case) the max latency is a signl we should not miss. If request are in the Application Footnotes Theo Schlossnagle. Time, but faster. 2017. URL: https://queue.acm.org/detail.cfm?id=3036398 (visited on 2021-08-21). \u21a9","title":"How to measure Latency?"},{"location":"#how-to-measure-latency","text":"Heinrich Hartmann - P99 CONF - 2021-10-06","title":"How to measure Latency?"},{"location":"#introduction","text":"Inspiration for this talk comes from Gil Tene - How (NOT) to Measure Latency? Slides (London 2013) / Video (StrangeLoop 2015) / Blog - HighScalability 2015 I have been talking about Statistics and Lantency for the last years: State of the Histogram (SLOConf 2021) / Circllhist (paper) / Latency SLOs Done Right (FOSDEM 2019) / Statistics for Enginees (2014..2019)","title":"Introduction"},{"location":"#plan-for-today","text":"How to measure Latency? Where to measure Latency? How to store Latency Data? How to analyze Latency Data?","title":"Plan for Today"},{"location":"#applications","text":"Montioring Benchmarking","title":"Applications"},{"location":"#how-to-measure-latency_1","text":"Measuring Latency in code is relatively straight forward: t_start = time . now () # # code section you want to measure # latency = time . now () - t_start This is really all there is to it. There are a few things to watch out: Capture early returns / exceptions. Use: defer , try/catch/finally . Which clock is used? What Resolution? sec / ms / usec / ns Monotonic or affected by NTP resets? What is measured? System Time, Process Time Want: high resolution, monotonic, system time: Java System.nanoTime() C clock_gettime(CLOCK_MONOTONIC,...) / gethrtime Python time.monotonic() Measurement Overhead. Checking the time costs time. Depending on implementation this can be 30ns - 300ns for the C-level call 1 , plus function call overhead of the language runtime. Rule of thumb: OK to measure latencies of 0.1ms or larger. If you are taking more than three time measurements, it's likely a good idea to abstract the time measurement into an object or a decorator, that can be used as follows: @timed def do_some_things (): ... Some tracing libraries (e.g. OpenTelemetry ) provide such decorators out-of-the box.","title":"How to Measure Latency?"},{"location":"#measuring-latency-over-time","text":"When measuring latency over time, e.g. in a monitoring system, there are trickly interactions of the collection window with the request duration that we are capturing: Do we count processes that overlap the collection windows? IMG: Latencies over Time Best Practices: Count requests to the collection window when they were completed When shutting down the benchmark, wait for all requests to complete! Keep an eye on the active requests (metric) Capture max latency, if this is larger than collection window, we make serious mistakes. Option: count active requests with the elapsed duration at end-of-window.","title":"Measuring Latency over Time"},{"location":"#where-to-measure-latency","text":"This is the much trickier part! We are most interested in measuring timings of request-reply interactions in potentially distributed system (HTTP/gRPC API calls): +--------+ +-----------+ +-----------+ +--------+ | Client | -----> | Intermedy | > | Intermedy | ------> | Server | +--------+ +-----------+ +-----------+ +--------+ The latency that the client experiences is influenced by the whole chain of interactions. Tradeoff: Measuring close to client is more meaningful but hard Measuring at server is easy but can be extermely misleading Best: Measure Both -> Tracing. IMG: Server Span / Client Span","title":"Where to Measure Latency?"},{"location":"#hidden-queues","text":"The computer systems we are working with have a lot of hidden queues in the HTTP libraries, TCP/IP stack, NIC Cards, Network Gear, etc. this means that the latency that the client experiences may be much worse, than what can be measured on the server side. It's very hard to observe those queues directly, we have to measure at the client to make sure we capture the impacts of all of them. IMG: Hidden Queues We can model these systems as queuing systms with \"hidden\" queues.","title":"Hidden Queues"},{"location":"#request-time-vs-service-time","text":"It's very easy to confuse request time with service time. If you are measing latency at the server, you are measuring service time. Serice Time looks like Request Time when there is little traffic Service Time can look completely FINE even if your clients have TERRIBLE experience. In Queuing Systems, this can have drastic consequences. IMG: Server with Queue Let's do some analysis...","title":"Request Time vs. Service Time"},{"location":"#coordinated-omission-in-benchmarking","text":"In benchmarking applications there are are a surprising number of cases, where the load generator coordinates with the system under test, and thereby skews latency measurements. Examples: Load is generated in loop like so: while True : start = time . now () request . get ( \"some.api/endpoint\" ) record ( time . now () - start ) This logic backs-up as soon as the requests take longer. You are effectively measuring service-time of a single worker. When latency is dominated by GC activity (e.g. Java) and the load generator runs in the in the same process, it will be affected by GC pauses as well, and hence miss requests when the system is stalled.","title":"Coordinated Omission in Benchmarking"},{"location":"#how-to-store-latency-data","text":"Requirements: Can compute percentiles Can merge latency data over longer time periods Options: Raw Data (logs, events, traces) Histograms (i.e. Mergable Summaries, Quantile Digest, etc.)","title":"How to Store Latency Data?"},{"location":"#how-to-analyze-latency-data","text":"Look at the max latency. Stalls of the system, often only affect a low number of requests. They still cause queuing in the upstream systems. Hence, when measuring service-time (which is often the case) the max latency is a signl we should not miss. If request are in the","title":"How to Analyze Latency Data"},{"location":"#application","text":"","title":"Application"},{"location":"#footnotes","text":"Theo Schlossnagle. Time, but faster. 2017. URL: https://queue.acm.org/detail.cfm?id=3036398 (visited on 2021-08-21). \u21a9","title":"Footnotes"},{"location":"service-time/","text":"Request Time vs. Service Time It's very easy to confuse request time with service time. In Queuing Systems, this can have drastic consequences. IMG: Server with Queue If you are measing latency at the server, you are measuring service time. Serice Time looks like Request Time when there is little traffic Service Time can look completely FINE even if your clients have TERRIBLE experience. Cheating Twice: Catch-up after stall, and","title":"Request Time vs. Service Time"},{"location":"service-time/#request-time-vs-service-time","text":"It's very easy to confuse request time with service time. In Queuing Systems, this can have drastic consequences. IMG: Server with Queue If you are measing latency at the server, you are measuring service time. Serice Time looks like Request Time when there is little traffic Service Time can look completely FINE even if your clients have TERRIBLE experience. Cheating Twice: Catch-up after stall, and","title":"Request Time vs. Service Time"}],"index":{"fieldVectors":[["title/",[0,0.031,1,0.127]],["text/",[0,0.048,1,0.196,2,0.506,3,0.506,4,0.506,5,0.506,6,0.864,7,0.506,8,0.506,9,0.506,10,0.506,11,0.906,12,0.506,13,0.506,14,0.506,15,0.506,16,0.506,17,0.506,18,2.01,19,0.506,20,0.506,21,0.906,22,0.506,23,0.506,24,0.906,25,0.506,26,0.506,27,0.506,28,0.506,29,0.751,30,0.506,31,0.506,32,0.506,33,0.506,34,0.506,35,0.506,36,0.506,37,0.506,38,0.506,39,0.506,40,0.506,41,0.506,42,0.751,43,1.152,44,0.751,45,0.864,46,0.506,47,0.9,48,0.906,49,0.506,50,0.506,51,0.506,52,0.906,53,0.259,54,1.244,55,0.506,56,0.906,57,0.506,58,0.506,59,0.506,60,0.506,61,0.906,62,1.053,63,0.506,64,0.506,65,0.506,66,1.23,67,0.506,68,0.506,69,0.506,70,0.906,71,0.506,72,0.506,73,0.506,74,0.506,75,0.906,76,0.864,77,0.506,78,0.506,79,0.702,80,0.864,81,0.506,82,0.751,83,0.506,84,0.906,85,0.506,86,0.506,87,0.506,88,0.506,89,0.906,90,0.506,91,0.506,92,0.506,93,0.506,94,0.506,95,0.506,96,0.506,97,1.021,98,0.506,99,0.506,100,0.506,101,0.506,102,0.506,103,0.506,104,0.506,105,0.506,106,0.506,107,0.751,108,0.751,109,0.751,110,0.506,111,0.635,112,0.506,113,0.506,114,0.506,115,0.506,116,0.906,117,0.506,118,0.506,119,0.506,120,0.864,121,0.751,122,0.864,123,0.506,124,0.506,125,0.506,126,0.506,127,1.053,128,0.506,129,0.506,130,1.021,131,1.499,132,1.725,133,0.639,134,0.906,135,1.23,136,0.506,137,0.665,138,0.751,139,0.506,140,0.739,141,0.506,142,0.506,143,0.506,144,0.506,145,0.506,146,1.021,147,0.506,148,1.021,149,0.751,150,0.506,151,0.506,152,0.751,153,0.506,154,0.506,155,0.751,156,0.506,157,0.506,158,0.506,159,0.506,160,0.506,161,0.506,162,0.506,163,0.506,164,1.075,165,0.906,166,0.989,167,0.635,168,0.506,169,0.506,170,0.506,171,0.506,172,0.506,173,0.506,174,0.751,175,0.544,176,0.506,177,0.506,178,0.506,179,0.906,180,1.244,181,0.989,182,0.751,183,0.506,184,0.506,185,0.506,186,0.506,187,0.506,188,0.506,189,0.506,190,0.506,191,0.506,192,0.751,193,0.506,194,0.506,195,0.506,196,0.544,197,0.506,198,0.506,199,0.506,200,0.506,201,0.506,202,0.506,203,0.635,204,0.506,205,0.304,206,0.621,207,0.355,208,0.355,209,0.355,210,0.739,211,0.355,212,0.355,213,0.355,214,0.355,215,0.355,216,0.355,217,0.355,218,0.506,219,0.506,220,0.751,221,0.506,222,0.506,223,0.751,224,0.751,225,1.23,226,1.23,227,0.506,228,0.506,229,0.506,230,0.506,231,0.506,232,0.506,233,0.506,234,0.906,235,0.506,236,0.506,237,0.506,238,0.506,239,0.355,240,0.506,241,0.751,242,0.506,243,0.506,244,0.506,245,0.506,246,0.906,247,0.506,248,0.506,249,0.506,250,0.506,251,0.751,252,0.751,253,0.544,254,0.506,255,0.506,256,0.506,257,0.506,258,0.506,259,0.506,260,0.506,261,0.506,262,0.506,263,0.506,264,0.506,265,0.506,266,0.506,267,0.506,268,0.506,269,0.506,270,0.506,271,0.506,272,0.506,273,0.506,274,0.506,275,0.506,276,0.506,277,0.506,278,0.506,279,0.506,280,0.506]],["title/#how-to-measure-latency",[0,0.031,1,0.127]],["text/#how-to-measure-latency",[2,3.127,3,3.127,4,3.127,5,3.127,6,2.196,7,3.127,8,3.127]],["title/#introduction",[9,2.636]],["text/#introduction",[0,0.034,1,0.18,6,1.676,10,2.387,11,3.08,12,2.387,13,2.387,14,2.387,15,2.387,16,2.387,17,2.387,18,1.925,19,2.387,20,2.387,21,3.08,22,2.387,23,2.387,24,3.08,25,2.387,26,2.387,27,2.387,28,2.387,29,1.98,30,2.387,31,2.387,32,2.387,33,2.387,34,2.387,35,2.387,36,2.387,37,2.387,38,2.387,39,2.387]],["title/#plan-for-today",[40,2.176,41,2.176]],["text/#plan-for-today",[0,0.051,1,0.229,42,2.525,43,2.15,44,2.525]],["title/#applications",[45,1.851]],["text/#applications",[46,3.277,47,1.968]],["title/#how-to-measure-latency_1",[0,0.031,1,0.127]],["text/#how-to-measure-latency_1",[0,0.048,1,0.153,18,2.04,48,2.186,49,1.461,50,1.461,51,1.461,52,2.186,53,0.275,54,1.814,55,1.461,56,2.186,57,1.461,58,1.461,59,1.461,60,1.461,61,2.186,62,1.026,63,1.461,64,1.461,65,1.461,66,2.62,67,1.461,68,1.461,69,1.461,70,2.186,71,1.461,72,1.461,73,1.461,74,1.461,75,2.186,76,1.026,77,1.461,78,1.461,79,0.597,80,1.026,81,1.461,82,1.212,83,1.461,84,2.186,85,1.461,86,1.461,87,1.461,88,1.461,89,2.186,90,1.461,91,1.461,92,1.461,93,1.461,94,1.461,95,1.461,96,1.461,97,1.814,98,1.461,99,1.461,100,1.461,101,1.461,102,1.461,103,1.461,104,1.461,105,1.461,106,1.461,107,1.212,108,1.212,109,1.212,110,1.461,111,0.754,112,1.461,113,1.461,114,1.461,115,1.461,116,2.186,117,1.461,118,1.461,119,1.461,120,1.026,121,1.212,122,1.026,123,1.461,124,1.461,125,1.461,126,1.461]],["title/#measuring-latency-over-time",[0,0.023,1,0.094,53,0.122,127,1.133]],["text/#measuring-latency-over-time",[0,0.03,1,0.188,47,1.282,53,0.217,62,2.01,79,0.583,80,1.499,107,1.77,122,1.499,127,2.01,128,2.134,129,2.134,130,1.77,131,3.45,132,3.598,133,0.817,134,2.862,135,3.229,136,2.134,137,0.946,138,1.77,139,2.134,140,1.719,141,2.134,142,2.134,143,2.134,144,2.134,145,2.134,146,2.374,147,2.134,148,1.77,149,1.77,150,2.134,151,2.134,152,1.77,153,2.134,154,2.134]],["title/#where-to-measure-latency",[0,0.031,1,0.127]],["text/#where-to-measure-latency",[0,0.047,1,0.112,18,2.147,53,0.145,79,0.524,97,1.592,109,1.592,120,1.347,130,2.208,133,0.435,137,0.851,138,1.592,155,1.592,156,1.919,157,1.919,158,1.919,159,1.919,160,1.919,161,1.919,162,1.919,163,1.919,164,1.703,165,2.662,166,1.577,167,0.99,168,1.919,169,1.919,170,1.919,171,1.919,172,1.919,173,1.919,174,1.592,175,1.152,176,1.919,177,1.919,178,1.919,179,2.662]],["title/#hidden-queues",[180,1.805,181,1.123]],["text/#hidden-queues",[0,0.043,1,0.134,62,1.62,79,0.823,111,1.19,121,1.913,137,1.023,149,1.913,155,1.913,164,1.554,166,1.19,167,1.19,174,1.913,180,2.783,181,1.836,182,1.913,183,2.306,184,2.306,185,2.306,186,2.306,187,2.306,188,2.306,189,2.306,190,2.306,191,2.306,192,1.913,193,2.306,194,2.306,195,2.306,196,1.385,197,2.306,198,2.306,199,2.306,200,2.306,201,2.306,202,2.306,203,1.19,204,2.306]],["title/#request-time-vs-service-time",[53,0.163,133,0.324,205,0.858,206,0.463]],["text/#request-time-vs-service-time",[0,0.035,1,0.141,53,0.291,79,0.661,111,1.249,133,0.705,137,1.073,140,1.454,164,1.249,166,1.604,167,1.249,175,1.454,181,1.249,196,1.454,203,1.249,206,1.112,207,1.7,208,1.7,209,1.7,210,1.867,211,1.7,212,1.7,213,1.7,214,1.7,215,1.7,216,1.7,217,1.7,218,2.42,219,2.42]],["title/#coordinated-omission-in-benchmarking",[47,1.113,220,1.538,221,1.853]],["text/#coordinated-omission-in-benchmarking",[0,0.038,1,0.157,18,2.016,45,1.371,47,1.172,53,0.234,54,2.234,76,1.371,79,0.735,80,1.371,82,1.619,108,1.619,122,1.371,133,0.7,146,1.619,206,0.632,220,1.619,222,1.952,223,1.619,224,1.619,225,3.084,226,3.084,227,1.952,228,1.952,229,1.952,230,1.952,231,1.952,232,1.952,233,1.952,234,2.693,235,1.952,236,1.952,237,1.952,238,1.952,239,1.371,240,1.952,241,1.619,242,1.952,243,1.952,244,1.952,245,1.952,246,2.693,247,1.952,248,1.952,249,1.952,250,1.952,251,1.619,252,1.619,253,1.172]],["title/#how-to-store-latency-data",[1,0.108,42,1.538,43,1.113]],["text/#how-to-store-latency-data",[1,0.159,29,2.264,43,2.013,53,0.207,120,1.916,127,1.916,152,2.264,182,2.264,192,2.264,241,2.264,254,2.729,255,2.729,256,2.729,257,2.729,258,2.729,259,2.729,260,2.729,261,2.729,262,2.729,263,2.729,264,2.729,265,2.729]],["title/#how-to-analyze-latency-data",[1,0.108,43,1.113,44,1.538]],["text/#how-to-analyze-latency-data",[0,0.039,1,0.194,53,0.205,76,1.901,79,0.911,133,0.757,148,2.767,203,1.397,206,0.876,210,1.626,223,2.246,224,2.246,251,2.246,252,2.246,253,1.626,266,2.707,267,2.707,268,2.707,269,2.707,270,2.707]],["title/#application",[45,1.851]],["text/#application",[]],["title/#footnotes",[271,2.636]],["text/#footnotes",[6,2.1,18,1.543,53,0.227,272,2.991,273,2.991,274,2.991,275,2.991,276,2.991,277,2.991,278,2.991,279,2.991,280,2.991]],["title/service-time/",[53,0.163,133,0.324,205,0.858,206,0.463]],["text/service-time/",[0,0.033,1,0.134,53,0.296,79,0.625,111,1.182,133,0.759,137,1.016,140,1.376,164,1.182,166,1.548,167,1.182,175,1.376,181,1.182,196,1.376,203,1.182,205,1.376,206,1.149,207,1.609,208,1.609,209,1.609,210,1.802,211,1.609,212,1.609,213,1.609,214,1.609,215,1.609,216,1.609,217,1.609,239,1.609,253,1.376,281,2.291,282,2.291,283,2.291]],["title/service-time/#request-time-vs-service-time",[53,0.163,133,0.324,205,0.858,206,0.463]],["text/service-time/#request-time-vs-service-time",[0,0.034,1,0.138,53,0.289,79,0.647,111,1.223,133,0.696,137,1.051,140,1.423,164,1.223,166,1.582,167,1.223,175,1.423,181,1.223,196,1.423,203,1.223,206,1.101,207,1.664,208,1.664,209,1.664,210,1.842,211,1.664,212,1.664,213,1.664,214,1.664,215,1.664,216,1.664,217,1.664,239,1.664,253,1.423,281,2.37,282,2.37,283,2.37]]],"fields":["title","text"],"invertedIndex":[["",{"_index":18,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"#footnotes":{},"#how-to-measure-latency_1":{},"#introduction":{},"#where-to-measure-latency":{}},"title":{}}],["0.1m",{"_index":106,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["06",{"_index":8,"text":{"":{},"#how-to-measure-latency":{}},"title":{}}],["08",{"_index":279,"text":{"":{},"#footnotes":{}},"title":{}}],["1",{"_index":98,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["10",{"_index":7,"text":{"":{},"#how-to-measure-latency":{}},"title":{}}],["2013",{"_index":17,"text":{"":{},"#introduction":{}},"title":{}}],["2014..2019",{"_index":39,"text":{"":{},"#introduction":{}},"title":{}}],["2015",{"_index":21,"text":{"":{},"#introduction":{}},"title":{}}],["2017",{"_index":275,"text":{"":{},"#footnotes":{}},"title":{}}],["2019",{"_index":37,"text":{"":{},"#introduction":{}},"title":{}}],["2021",{"_index":6,"text":{"":{},"#footnotes":{},"#how-to-measure-latency":{},"#introduction":{}},"title":{}}],["21",{"_index":280,"text":{"":{},"#footnotes":{}},"title":{}}],["300n",{"_index":95,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["30n",{"_index":94,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["abstract",{"_index":114,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["activ",{"_index":146,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"#measuring-latency-over-time":{}},"title":{}}],["affect",{"_index":76,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"#how-to-analyze-latency-data":{},"#how-to-measure-latency_1":{}},"title":{}}],["analysi",{"_index":219,"text":{"":{},"#request-time-vs-service-time":{}},"title":{}}],["analyz",{"_index":44,"text":{"":{},"#plan-for-today":{}},"title":{"#how-to-analyze-latency-data":{}}}],["api",{"_index":163,"text":{"":{},"#where-to-measure-latency":{}},"title":{}}],["applic",{"_index":45,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{"#application":{},"#applications":{}}}],["back",{"_index":238,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["benchmark",{"_index":47,"text":{"":{},"#applications":{},"#coordinated-omission-in-benchmarking":{},"#measuring-latency-over-time":{}},"title":{"#coordinated-omission-in-benchmarking":{}}}],["best",{"_index":138,"text":{"":{},"#measuring-latency-over-time":{},"#where-to-measure-latency":{}},"title":{}}],["blog",{"_index":22,"text":{"":{},"#introduction":{}},"title":{}}],["both",{"_index":178,"text":{"":{},"#where-to-measure-latency":{}},"title":{}}],["box",{"_index":126,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["c",{"_index":84,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["call",{"_index":97,"text":{"":{},"#how-to-measure-latency_1":{},"#where-to-measure-latency":{}},"title":{}}],["captur",{"_index":62,"text":{"":{},"#hidden-queues":{},"#how-to-measure-latency_1":{},"#measuring-latency-over-time":{}},"title":{}}],["card",{"_index":189,"text":{"":{},"#hidden-queues":{}},"title":{}}],["case",{"_index":224,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"#how-to-analyze-latency-data":{}},"title":{}}],["catch",{"_index":283,"text":{"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["caus",{"_index":268,"text":{"":{},"#how-to-analyze-latency-data":{}},"title":{}}],["chain",{"_index":170,"text":{"":{},"#where-to-measure-latency":{}},"title":{}}],["cheat",{"_index":281,"text":{"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["check",{"_index":90,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["circllhist",{"_index":31,"text":{"":{},"#introduction":{}},"title":{}}],["client",{"_index":164,"text":{"":{},"#hidden-queues":{},"#request-time-vs-service-time":{},"#where-to-measure-latency":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["clock",{"_index":69,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["clock_gettime(clock_monoton",{"_index":85,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["close",{"_index":172,"text":{"":{},"#where-to-measure-latency":{}},"title":{}}],["code",{"_index":48,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["collect",{"_index":131,"text":{"":{},"#measuring-latency-over-time":{}},"title":{}}],["come",{"_index":12,"text":{"":{},"#introduction":{}},"title":{}}],["complet",{"_index":140,"text":{"":{},"#measuring-latency-over-time":{},"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["comput",{"_index":182,"text":{"":{},"#hidden-queues":{},"#how-to-store-latency-data":{}},"title":{}}],["conf",{"_index":5,"text":{"":{},"#how-to-measure-latency":{}},"title":{}}],["confus",{"_index":207,"text":{"":{},"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["consequ",{"_index":217,"text":{"":{},"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["coordin",{"_index":220,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{"#coordinated-omission-in-benchmarking":{}}}],["cost",{"_index":91,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["count",{"_index":135,"text":{"":{},"#measuring-latency-over-time":{}},"title":{}}],["data",{"_index":43,"text":{"":{},"#how-to-store-latency-data":{},"#plan-for-today":{}},"title":{"#how-to-analyze-latency-data":{},"#how-to-store-latency-data":{}}}],["decor",{"_index":116,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["def",{"_index":118,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["defer",{"_index":67,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["depend",{"_index":92,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["digest",{"_index":265,"text":{"":{},"#how-to-store-latency-data":{}},"title":{}}],["directli",{"_index":199,"text":{"":{},"#hidden-queues":{}},"title":{}}],["distribut",{"_index":161,"text":{"":{},"#where-to-measure-latency":{}},"title":{}}],["do_some_th",{"_index":119,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["domin",{"_index":245,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["done",{"_index":34,"text":{"":{},"#introduction":{}},"title":{}}],["down",{"_index":142,"text":{"":{},"#measuring-latency-over-time":{}},"title":{}}],["drastic",{"_index":216,"text":{"":{},"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["durat",{"_index":134,"text":{"":{},"#measuring-latency-over-time":{}},"title":{}}],["e.g",{"_index":122,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"#how-to-measure-latency_1":{},"#measuring-latency-over-time":{}},"title":{}}],["earli",{"_index":63,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["easi",{"_index":175,"text":{"":{},"#request-time-vs-service-time":{},"#where-to-measure-latency":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["effect",{"_index":242,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["elaps",{"_index":153,"text":{"":{},"#measuring-latency-over-time":{}},"title":{}}],["end",{"_index":154,"text":{"":{},"#measuring-latency-over-time":{}},"title":{}}],["engine",{"_index":38,"text":{"":{},"#introduction":{}},"title":{}}],["etc",{"_index":192,"text":{"":{},"#hidden-queues":{},"#how-to-store-latency-data":{}},"title":{}}],["even",{"_index":214,"text":{"":{},"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["event",{"_index":260,"text":{"":{},"#how-to-store-latency-data":{}},"title":{}}],["exampl",{"_index":231,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["except",{"_index":65,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["experi",{"_index":167,"text":{"":{},"#hidden-queues":{},"#request-time-vs-service-time":{},"#where-to-measure-latency":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["exterm",{"_index":176,"text":{"":{},"#where-to-measure-latency":{}},"title":{}}],["ey",{"_index":145,"text":{"":{},"#measuring-latency-over-time":{}},"title":{}}],["faster",{"_index":274,"text":{"":{},"#footnotes":{}},"title":{}}],["few",{"_index":58,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["fine",{"_index":213,"text":{"":{},"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["follow",{"_index":117,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["footnot",{"_index":271,"text":{"":{}},"title":{"#footnotes":{}}}],["forward",{"_index":51,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["fosdem",{"_index":36,"text":{"":{},"#introduction":{}},"title":{}}],["function",{"_index":100,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["gc",{"_index":246,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["gear",{"_index":191,"text":{"":{},"#hidden-queues":{}},"title":{}}],["gener",{"_index":226,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["gethrtim",{"_index":86,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["gil",{"_index":13,"text":{"":{},"#introduction":{}},"title":{}}],["good",{"_index":112,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["hard",{"_index":174,"text":{"":{},"#hidden-queues":{},"#where-to-measure-latency":{}},"title":{}}],["hartmann",{"_index":3,"text":{"":{},"#how-to-measure-latency":{}},"title":{}}],["heinrich",{"_index":2,"text":{"":{},"#how-to-measure-latency":{}},"title":{}}],["henc",{"_index":251,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"#how-to-analyze-latency-data":{}},"title":{}}],["hidden",{"_index":180,"text":{"":{},"#hidden-queues":{}},"title":{"#hidden-queues":{}}}],["high",{"_index":81,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["highscal",{"_index":23,"text":{"":{},"#introduction":{}},"title":{}}],["histogram",{"_index":29,"text":{"":{},"#how-to-store-latency-data":{},"#introduction":{}},"title":{}}],["http",{"_index":185,"text":{"":{},"#hidden-queues":{}},"title":{}}],["http/grpc",{"_index":162,"text":{"":{},"#where-to-measure-latency":{}},"title":{}}],["https://queue.acm.org/detail.cfm?id=3036398",{"_index":277,"text":{"":{},"#footnotes":{}},"title":{}}],["i.",{"_index":261,"text":{"":{},"#how-to-store-latency-data":{}},"title":{}}],["idea",{"_index":113,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["img",{"_index":137,"text":{"":{},"#hidden-queues":{},"#measuring-latency-over-time":{},"#request-time-vs-service-time":{},"#where-to-measure-latency":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["impact",{"_index":201,"text":{"":{},"#hidden-queues":{}},"title":{}}],["implement",{"_index":93,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["influenc",{"_index":168,"text":{"":{},"#where-to-measure-latency":{}},"title":{}}],["inspir",{"_index":10,"text":{"":{},"#introduction":{}},"title":{}}],["interact",{"_index":130,"text":{"":{},"#measuring-latency-over-time":{},"#where-to-measure-latency":{}},"title":{}}],["interest",{"_index":158,"text":{"":{},"#where-to-measure-latency":{}},"title":{}}],["intermedi",{"_index":165,"text":{"":{},"#where-to-measure-latency":{}},"title":{}}],["introduct",{"_index":9,"text":{"":{}},"title":{"#introduction":{}}}],["it'",{"_index":111,"text":{"":{},"#hidden-queues":{},"#how-to-measure-latency_1":{},"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["java",{"_index":82,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"#how-to-measure-latency_1":{}},"title":{}}],["keep",{"_index":144,"text":{"":{},"#measuring-latency-over-time":{}},"title":{}}],["languag",{"_index":101,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["lantenc",{"_index":25,"text":{"":{},"#introduction":{}},"title":{}}],["larger",{"_index":107,"text":{"":{},"#how-to-measure-latency_1":{},"#measuring-latency-over-time":{}},"title":{}}],["last",{"_index":26,"text":{"":{},"#introduction":{}},"title":{}}],["latenc",{"_index":1,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"#hidden-queues":{},"#how-to-analyze-latency-data":{},"#how-to-measure-latency_1":{},"#how-to-store-latency-data":{},"#introduction":{},"#measuring-latency-over-time":{},"#plan-for-today":{},"#request-time-vs-service-time":{},"#where-to-measure-latency":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{"":{},"#how-to-analyze-latency-data":{},"#how-to-measure-latency":{},"#how-to-measure-latency_1":{},"#how-to-store-latency-data":{},"#measuring-latency-over-time":{},"#where-to-measure-latency":{}}}],["let'",{"_index":218,"text":{"":{},"#request-time-vs-service-time":{}},"title":{}}],["level",{"_index":96,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["librari",{"_index":121,"text":{"":{},"#hidden-queues":{},"#how-to-measure-latency_1":{}},"title":{}}],["littl",{"_index":211,"text":{"":{},"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["load",{"_index":225,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["log",{"_index":259,"text":{"":{},"#how-to-store-latency-data":{}},"title":{}}],["logic",{"_index":237,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["london",{"_index":16,"text":{"":{},"#introduction":{}},"title":{}}],["longer",{"_index":241,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"#how-to-store-latency-data":{}},"title":{}}],["look",{"_index":210,"text":{"":{},"#how-to-analyze-latency-data":{},"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["loop",{"_index":232,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["lot",{"_index":184,"text":{"":{},"#hidden-queues":{}},"title":{}}],["low",{"_index":266,"text":{"":{},"#how-to-analyze-latency-data":{}},"title":{}}],["make",{"_index":149,"text":{"":{},"#hidden-queues":{},"#measuring-latency-over-time":{}},"title":{}}],["max",{"_index":148,"text":{"":{},"#how-to-analyze-latency-data":{},"#measuring-latency-over-time":{}},"title":{}}],["mean",{"_index":193,"text":{"":{},"#hidden-queues":{}},"title":{}}],["meaning",{"_index":173,"text":{"":{},"#where-to-measure-latency":{}},"title":{}}],["meas",{"_index":208,"text":{"":{},"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["measur",{"_index":0,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"#hidden-queues":{},"#how-to-analyze-latency-data":{},"#how-to-measure-latency_1":{},"#introduction":{},"#measuring-latency-over-time":{},"#plan-for-today":{},"#request-time-vs-service-time":{},"#where-to-measure-latency":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{"":{},"#how-to-measure-latency":{},"#how-to-measure-latency_1":{},"#measuring-latency-over-time":{},"#where-to-measure-latency":{}}}],["merg",{"_index":256,"text":{"":{},"#how-to-store-latency-data":{}},"title":{}}],["mergabl",{"_index":262,"text":{"":{},"#how-to-store-latency-data":{}},"title":{}}],["metric",{"_index":147,"text":{"":{},"#measuring-latency-over-time":{}},"title":{}}],["mislead",{"_index":177,"text":{"":{},"#where-to-measure-latency":{}},"title":{}}],["miss",{"_index":252,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"#how-to-analyze-latency-data":{}},"title":{}}],["mistak",{"_index":151,"text":{"":{},"#measuring-latency-over-time":{}},"title":{}}],["model",{"_index":202,"text":{"":{},"#hidden-queues":{}},"title":{}}],["monitor",{"_index":128,"text":{"":{},"#measuring-latency-over-time":{}},"title":{}}],["monoton",{"_index":75,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["montior",{"_index":46,"text":{"":{},"#applications":{}},"title":{}}],["more",{"_index":109,"text":{"":{},"#how-to-measure-latency_1":{},"#where-to-measure-latency":{}},"title":{}}],["ms",{"_index":72,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["much",{"_index":155,"text":{"":{},"#hidden-queues":{},"#where-to-measure-latency":{}},"title":{}}],["network",{"_index":190,"text":{"":{},"#hidden-queues":{}},"title":{}}],["nic",{"_index":188,"text":{"":{},"#hidden-queues":{}},"title":{}}],["now",{"_index":54,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"#how-to-measure-latency_1":{}},"title":{}}],["ns",{"_index":74,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["ntp",{"_index":77,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["number",{"_index":223,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"#how-to-analyze-latency-data":{}},"title":{}}],["object",{"_index":115,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["observ",{"_index":197,"text":{"":{},"#hidden-queues":{}},"title":{}}],["ok",{"_index":105,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["omiss",{"_index":221,"text":{"":{}},"title":{"#coordinated-omission-in-benchmarking":{}}}],["opentelemetri",{"_index":123,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["option",{"_index":152,"text":{"":{},"#how-to-store-latency-data":{},"#measuring-latency-over-time":{}},"title":{}}],["out",{"_index":61,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["over",{"_index":127,"text":{"":{},"#how-to-store-latency-data":{},"#measuring-latency-over-time":{}},"title":{"#measuring-latency-over-time":{}}}],["overhead",{"_index":89,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["overlap",{"_index":136,"text":{"":{},"#measuring-latency-over-time":{}},"title":{}}],["p99",{"_index":4,"text":{"":{},"#how-to-measure-latency":{}},"title":{}}],["paper",{"_index":32,"text":{"":{},"#introduction":{}},"title":{}}],["part",{"_index":157,"text":{"":{},"#where-to-measure-latency":{}},"title":{}}],["paus",{"_index":249,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["percentil",{"_index":255,"text":{"":{},"#how-to-store-latency-data":{}},"title":{}}],["period",{"_index":257,"text":{"":{},"#how-to-store-latency-data":{}},"title":{}}],["plan",{"_index":40,"text":{"":{}},"title":{"#plan-for-today":{}}}],["plu",{"_index":99,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["potenti",{"_index":160,"text":{"":{},"#where-to-measure-latency":{}},"title":{}}],["practic",{"_index":139,"text":{"":{},"#measuring-latency-over-time":{}},"title":{}}],["process",{"_index":80,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"#how-to-measure-latency_1":{},"#measuring-latency-over-time":{}},"title":{}}],["provid",{"_index":124,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["python",{"_index":87,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["quantil",{"_index":264,"text":{"":{},"#how-to-store-latency-data":{}},"title":{}}],["queu",{"_index":203,"text":{"":{},"#hidden-queues":{},"#how-to-analyze-latency-data":{},"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["queue",{"_index":181,"text":{"":{},"#hidden-queues":{},"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{"#hidden-queues":{}}}],["raw",{"_index":258,"text":{"":{},"#how-to-store-latency-data":{}},"title":{}}],["realli",{"_index":57,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["record",{"_index":236,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["rel",{"_index":49,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["repli",{"_index":159,"text":{"":{},"#where-to-measure-latency":{}},"title":{}}],["request",{"_index":133,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"#how-to-analyze-latency-data":{},"#measuring-latency-over-time":{},"#request-time-vs-service-time":{},"#where-to-measure-latency":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}}}],["requir",{"_index":254,"text":{"":{},"#how-to-store-latency-data":{}},"title":{}}],["reset",{"_index":78,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["resolut",{"_index":70,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["return",{"_index":64,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["right",{"_index":35,"text":{"":{},"#introduction":{}},"title":{}}],["rule",{"_index":103,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["run",{"_index":247,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["runtim",{"_index":102,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["same",{"_index":248,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["schlossnagl",{"_index":273,"text":{"":{},"#footnotes":{}},"title":{}}],["sec",{"_index":71,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["section",{"_index":55,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["seric",{"_index":209,"text":{"":{},"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["seriou",{"_index":150,"text":{"":{},"#measuring-latency-over-time":{}},"title":{}}],["server",{"_index":166,"text":{"":{},"#hidden-queues":{},"#request-time-vs-service-time":{},"#where-to-measure-latency":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["servic",{"_index":206,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"#how-to-analyze-latency-data":{},"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}}}],["shut",{"_index":141,"text":{"":{},"#measuring-latency-over-time":{}},"title":{}}],["side",{"_index":195,"text":{"":{},"#hidden-queues":{}},"title":{}}],["signl",{"_index":270,"text":{"":{},"#how-to-analyze-latency-data":{}},"title":{}}],["singl",{"_index":243,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["skew",{"_index":230,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["slide",{"_index":15,"text":{"":{},"#introduction":{}},"title":{}}],["slo",{"_index":33,"text":{"":{},"#introduction":{}},"title":{}}],["sloconf",{"_index":30,"text":{"":{},"#introduction":{}},"title":{}}],["some.api/endpoint",{"_index":235,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["soon",{"_index":240,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["span",{"_index":179,"text":{"":{},"#where-to-measure-latency":{}},"title":{}}],["stack",{"_index":187,"text":{"":{},"#hidden-queues":{}},"title":{}}],["stall",{"_index":253,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"#how-to-analyze-latency-data":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["start",{"_index":234,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["state",{"_index":28,"text":{"":{},"#introduction":{}},"title":{}}],["statist",{"_index":24,"text":{"":{},"#introduction":{}},"title":{}}],["still",{"_index":267,"text":{"":{},"#how-to-analyze-latency-data":{}},"title":{}}],["store",{"_index":42,"text":{"":{},"#plan-for-today":{}},"title":{"#how-to-store-latency-data":{}}}],["straight",{"_index":50,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["strangeloop",{"_index":20,"text":{"":{},"#introduction":{}},"title":{}}],["such",{"_index":125,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["summari",{"_index":263,"text":{"":{},"#how-to-store-latency-data":{}},"title":{}}],["sure",{"_index":200,"text":{"":{},"#hidden-queues":{}},"title":{}}],["surpris",{"_index":222,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["system",{"_index":79,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"#hidden-queues":{},"#how-to-analyze-latency-data":{},"#how-to-measure-latency_1":{},"#measuring-latency-over-time":{},"#request-time-vs-service-time":{},"#where-to-measure-latency":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["system.nanotim",{"_index":83,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["systm",{"_index":204,"text":{"":{},"#hidden-queues":{}},"title":{}}],["t_start",{"_index":52,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["take",{"_index":108,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"#how-to-measure-latency_1":{}},"title":{}}],["talk",{"_index":11,"text":{"":{},"#introduction":{}},"title":{}}],["tcp/ip",{"_index":186,"text":{"":{},"#hidden-queues":{}},"title":{}}],["tene",{"_index":14,"text":{"":{},"#introduction":{}},"title":{}}],["terribl",{"_index":215,"text":{"":{},"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["test",{"_index":228,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["theo",{"_index":272,"text":{"":{},"#footnotes":{}},"title":{}}],["therebi",{"_index":229,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["thing",{"_index":59,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["those",{"_index":198,"text":{"":{},"#hidden-queues":{}},"title":{}}],["three",{"_index":110,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["thumb",{"_index":104,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["time",{"_index":53,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"#footnotes":{},"#how-to-analyze-latency-data":{},"#how-to-measure-latency_1":{},"#how-to-store-latency-data":{},"#measuring-latency-over-time":{},"#request-time-vs-service-time":{},"#where-to-measure-latency":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{"#measuring-latency-over-time":{},"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}}}],["time.monoton",{"_index":88,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["today",{"_index":41,"text":{"":{}},"title":{"#plan-for-today":{}}}],["trace",{"_index":120,"text":{"":{},"#how-to-measure-latency_1":{},"#how-to-store-latency-data":{},"#where-to-measure-latency":{}},"title":{}}],["tradeoff",{"_index":171,"text":{"":{},"#where-to-measure-latency":{}},"title":{}}],["traffic",{"_index":212,"text":{"":{},"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["trickier",{"_index":156,"text":{"":{},"#where-to-measure-latency":{}},"title":{}}],["trickli",{"_index":129,"text":{"":{},"#measuring-latency-over-time":{}},"title":{}}],["true",{"_index":233,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["try/catch/fin",{"_index":68,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["twice",{"_index":282,"text":{"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["under",{"_index":227,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["up",{"_index":239,"text":{"":{},"#coordinated-omission-in-benchmarking":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["upstream",{"_index":269,"text":{"":{},"#how-to-analyze-latency-data":{}},"title":{}}],["url",{"_index":276,"text":{"":{},"#footnotes":{}},"title":{}}],["us",{"_index":66,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["usec",{"_index":73,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["veri",{"_index":196,"text":{"":{},"#hidden-queues":{},"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}},"title":{}}],["video",{"_index":19,"text":{"":{},"#introduction":{}},"title":{}}],["visit",{"_index":278,"text":{"":{},"#footnotes":{}},"title":{}}],["vs",{"_index":205,"text":{"":{},"service-time/":{}},"title":{"#request-time-vs-service-time":{},"service-time/":{},"service-time/#request-time-vs-service-time":{}}}],["wait",{"_index":143,"text":{"":{},"#measuring-latency-over-time":{}},"title":{}}],["want",{"_index":56,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["watch",{"_index":60,"text":{"":{},"#how-to-measure-latency_1":{}},"title":{}}],["well",{"_index":250,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["whole",{"_index":169,"text":{"":{},"#where-to-measure-latency":{}},"title":{}}],["window",{"_index":132,"text":{"":{},"#measuring-latency-over-time":{}},"title":{}}],["work",{"_index":183,"text":{"":{},"#hidden-queues":{}},"title":{}}],["worker",{"_index":244,"text":{"":{},"#coordinated-omission-in-benchmarking":{}},"title":{}}],["wors",{"_index":194,"text":{"":{},"#hidden-queues":{}},"title":{}}],["year",{"_index":27,"text":{"":{},"#introduction":{}},"title":{}}]],"pipeline":["stemmer"],"version":"2.3.9"}}